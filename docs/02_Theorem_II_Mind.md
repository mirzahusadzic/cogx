# **Part II: The Operational Framework (The "Mind")**

This part of the blueprint defines the operational logic of the system. It proves that the stateful body, whose existence and construction were proven in Theorem I, can be intelligently used, maintained, and adapted in response to new events and queries.

## **2. Theorem II: Soundness of Dynamic Operations and Optimal Context Retrieval**

Given a verifiably constructed Grounded Context Pool (PGC) (Th. I), it is possible to:

1. Reliably propagate state changes throughout the PGC in response to an external `change_event` via an `Update Function` (U) such that the system remains consistent.
2. Compose an Optimal Grounded Prompt Context (COGP) for any new query (Pnew) via a `Context Sampling Function` (Sigma) that is verifiably relevant and complete.

### **2.1. Proof Part A: Axioms and Foundational Definitions**

1. **[2.1.1] Axiom of Change:** Let a `change_event` be an external operation that alters the content of a Draw file, resulting in a new `object.hash` for its Path.
2. **[2.1.2] Axiom of Query:** Let Pnew be a new, arbitrary user query.

### **2.2. Proof Part B: The Meta-Language Framework**

1. **[2.2.1] The `cognition-cli` (O):** The operational logic is executed by an Orchestrator (O) which acts as an interpreter for a `cognition-script` program.
2. **[2.2.2] The `COGX.md` & `cognition-script`:** The `cognition-script` is a procedural, high-level language contained in `COGX.md` that defines the orchestration logic.
3. **[2.2.3] The "Living Language":** The interpreter's understanding of the script is grounded by its reference to the Language Specification (Lambda) and a Referential Implementation (Iref), both of which are GKes within the system, allowing the language itself to be versioned and evolved.

### **2.3. Proof Part C: The Core Dynamic Functions**

1. **[2.3.1] Definition of the Update Function (U):**
    Let U be a function that takes a `change_event` as input and modifies the state of the `index`. It is defined by a recursive algorithm, `Invalidate`, which is initiated with the `object.hash` of the changed raw file.
    * **The `Invalidate(object.hash)` function is defined as:**
        1. Identify all transformations that depend on the input `object.hash`. This is achieved by a direct lookup in the `reverse_deps` index: let `dependent.transforms` be the result of g(object.hash).        2. For each `transform` in `dependent.transforms`:
            a.  Identify all knowledge elements that were created by this transform by reading the `output.hashes` from the manifest of t.
            b.  For each `hash.out` in `output.hashes`:
                i.  Find the semantic `Path` p that currently points to this `hash.out` by querying the `index`.
                ii. Update the record for this `Path` p in the `index`, setting its `status` to `Invalidated`.
                iii. Recursively call `Invalidate(hash.out)` to continue the cascade.

2. **[2.3.2] Definition of the Context Sampling Function (Sigma):**
    Let Sigma be a function that takes a Pnew as input and returns a COGP. It is defined by a sequence of four distinct operations:
    * **Deconstruction:** The initial Pnew is processed by an LLM Agent (ALLM) to produce a structured `query.analysis` containing the query's core `entities`, `intent`, and implied `persona`.
    * **Anchor Identification:** The `index` is searched for `GK_e`s whose semantic `Path`s directly match the `entities` from the `query.analysis`. This yields a set of `anchors`.
    * **Context Expansion:** A graph traversal algorithm, `Traversal`, is executed starting from the `anchors`. This algorithm navigates the PGC by moving upward (to parent summaries), downward (to component members), sideways (by following dependency links in `transforms`), and horizontally (across overlays), collecting a set of `candidates`.
    * **Optimal Composition:** The final COGP is constructed by selecting a subset of the `candidates` that solves an optimization problem: maximizing the total `relevance` to the `query.analysis` while staying within a pre-defined `token_budget`.

### **2.4. Proof Part D: The Closed Loop of Context Plasticity**

These advanced operations are built upon the fundamental functions defined above.

1. **[2.4.1] The `in-mem-fs`:** An `in-mem-fs` is used as a high-speed cache of the PGC for the duration of a task.
2. **[2.4.2] The "Reaction Scene":** After a successful `Transform` operation, a "Reaction Scene" is generated by performing a localized, in-memory traversal (Sigma) from the point of change.
3. **[2.4.3] Emergent Planning:** This "Reaction Scene" is prepended to the context for the next LLM call, allowing the agent to dynamically adapt its plan based on the immediate consequences of its own actions.

### **2.5. Conclusion of Proof II:**

The `Update Function` (U) is a sound, recursive algorithm ensuring consistency via the `reverse_deps` index. The `Context Sampling Function` (Sigma) is a sound, multi-stage process ensuring the retrieval of a relevant and budget-constrained context. These functions provide the necessary foundation for advanced agentic behaviors like context plasticity. **Q.E.D.**
